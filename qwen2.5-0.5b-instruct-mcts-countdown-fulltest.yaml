seed: 42
log_level: INFO
logging_steps: 5
logging_strategy: steps

model_name: Qwen2.5-Coder-0.5B-Instruct
model_path: Qwen2.5-Coder-0.5B-Instruct
work_dir: ./exp/{model_name}-lora-grpo/{ts}/

countdown_conf: # evaluation
  numbers: [1, 2, 3, 4]
  target: 24
  problem_path: data/problem
  benchmark_problem_size: 4
  benchmark_iterations: 128
  evaluate_strategy: epoch
  benchmark_k_samples: 10

inf_conf:
  max_new_tokens: 512
  num_return_sequences: 2
  temperature: 0.3
  top_k: 30
  policy_prompt: "<|im_start|>system\nYou are a helpful assistant. You first think about the reasoning process in the mind and then provide the user with the answer.<|im_end|>\n<|im_start|>user\n Using the numbers {numbers}, create an equation that equals {target}. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags. Please output a best operation in format: <op> <index1> <index2> in the <answer> </answer> tags. <op> is the operator you can use, which are basic arithmetic operations (+, -, *, /). <index1> and <index2> are two different indexs smaller than {len_num}. For example <answer> * 0 1 </answer>.<|im_end|>\n<|im_start|>assistant\nLet me solve this step by step.\n<think>"
  value_prompt: "Given the numbers {numbers} and target {target}, rate the quality of operation {op} {index1} {index2} on a scale of 0-1. Answer:"

lora_conf:
  lora_rank: 8
  lora_alpha: 16
  target_modules:
    [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]
  lora_dropout: 0.05
  lora_bias: none
  task_type: CAUSAL_LM

train_conf:
  # data_path: ./data/rollout=1_iteration=128_samples.json
  optim: adamw_torch #paged_adamw_32bit #adamw
  lr: 0.000005
  lr_scheduler_type: cosine
  max_length: 512
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  logging_steps: 1
  num_generations: 2
  num_rulebase_only_epochs: 2
  num_samples_per_epoch: 1024
  data_selection_strategy: 'ppl' 
  data_selection_percent: 0.8 
  use_ppl_reward_shaping: True
  ppl_reward_alpha: 0.01 # PPL shaping的强度系数
  use_positional_reward_shaping: True
  positional_reward_gamma: 0.1     # (γ) 奖励的最大值 (强度)
  positional_reward_m_scaler: 15.0   # (m) 曲线的陡峭度
  positional_reward_n_shifter: -0.5  # (n) 曲线的平移 (n=-0.5意味着从序列中间开始奖励显著增加)
  reward_weights: [1.0, 0.2, 1.0, 0.1] 


mcts_conf: # training
  iterations: 512
  num_rollout: 100
  simulate_steps: 8
  c: 1.414

sample_conf:
  buffer_size: 1024
  pos_ratio: 0.5

